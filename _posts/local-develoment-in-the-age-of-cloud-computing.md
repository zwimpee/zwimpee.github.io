---
layout: post
comments: true
title:  "Transforming Machine Learning: A Foray into Rotationally Invariant GPTs"
excerpt: "> In a bid to capture symmetries and other properties within the data, we experiment with novel applications of Lie Groups to Neural Networks, introducing a twist in the tale - the Rotationally Invariant GPT."
date:   2023-06-15 08:00:00
mathjax: true
author: Zach Wimpee
thumbnail: /assets/intro/thumbnail.png
---

# From GPT to Rotationally Invariant GPT

At the heart of our experimentation lies the transformative idea of applying Lie Group theory to the realm of neural networks. As we work to understand smooth manifolds, we find ourselves moving away from defining transformations applied on objects purely in terms of local coordinates. 

In particular, we're delving into the properties of a group that is also a manifold - a Lie Group. A powerful mathematical concept, it provides a solid footing for our experiment as it naturally captures symmetries inherent in physical and data systems.

Our prime focus is on the GPT (Generative Pretraining Transformer) models, popularized by OpenAI. Known for their prowess in language modeling tasks, GPTs leverage the power of Transformers to learn the structure and semantic properties of input data.

## A Twist in the Tale: The Rotationally Invariant GPT

GPTs are incredibly powerful, but what if we could make them even better? Here's where the concept of rotational invariance comes into play. Simply put, a rotationally invariant system is one that remains unchanged under rotations. 

Our hypothesis is that by incorporating rotational invariance into the architecture of GPTs, the models can learn to better capture symmetries and other inherent properties within the data. We have named this new variant as the Rotationally Invariant GPT (RiGPT). 

A key part of our experiment involves implementing this idea and comparing the results of RiGPT with the traditional GPT. We are leveraging the power of PyTorch, a popular open-source machine learning library, for this purpose. 

## Conclusion: A Step into Uncharted Territory

This experiment is interesting for several reasons. First, it is a novel attempt at applying advanced mathematical concepts in the field of machine learning, thus potentially paving the way for a whole new breed of models. 

Second, it could result in more powerful and efficient GPT models that are better able to understand and generate natural language.

At the end of the day, we're not just looking to create better language models. We're striving to push the boundaries of what's possible in machine learning, and we're excited to see where this journey will take us. Stay tuned for the results of our experiment!
