# **Linear Algebra: The Foundation of Deep Learning**

## Introduction

*Linear* Algebra is a branch of mathematics that deals with vectors and matrices. It is a fundamental part of machine learning and deep learning. In this post, I will try to explain the basic concepts of linear algebra and how they are used in deep learning.

In this post, we will explore the fundamentals of this mathematical branch and how it forms the basis of deep learning. We'll also discuss the various operations related to matrices and vectors and their applications in deep learning, including familiar operations and alogrithms such as the dot product, matrix multiplication, cross products, etc.

Note that when we talk about the various matrix operations and their definitions that I will be treating these mathematics as first principles. That is, I will not be *motivating* the introduction of these concepts and abstractions in terms of their applications in machine learning. Instead, I want to help impart on you an *intuition* for the ways in which we can modify, compare, and combine these structures, which will then be used to motivate the *use* of these techniques in different parts of a model building pipeline.



## Scalars, Vectors, and Matrices

In linear algebra, we work with several basic building blocks that have specific properties:

- Scalars: Single numbers that can be real or complex
- Vectors: Ordered lists of numbers arranged in columns or rows
- Matrices: Rectangular arrays of numbers arranged in rows and columns

